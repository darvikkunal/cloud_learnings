{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33504d97",
   "metadata": {},
   "source": [
    "Day 2: Loops over Lists/Dicts (Pipeline Config Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c661d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic loaded :    survived  pclass                                               name  \\\n",
      "0         0       3                            Braund, Mr. Owen Harris   \n",
      "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2         1       3                             Heikkinen, Miss. Laina   \n",
      "\n",
      "      sex   age     fare  sibsp  parch  \n",
      "0    male  22.0   7.2500      1      0  \n",
      "1  female  38.0  71.2833      1      0  \n",
      "2  female  26.0   7.9250      0      0   \n",
      " 714 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/darvikkunalbanda/DATA_ENGINEERING/cloud_learnings/data/titanic.csv\")\n",
    "print(f\"Titanic loaded : {df.head(3)}\" , \"\\n\" ,len(df), \"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e9301",
   "metadata": {},
   "source": [
    "Exercise 1: Basic for loop over list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850f347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Pipeline sources ===== \n",
      "\n",
      "Loading passengers.csv...\n",
      " passengers.csv loaded (714 rows) \n",
      "\n",
      "Loading cabins.csv...\n",
      " cabins.csv loaded (714 rows) \n",
      "\n",
      "Loading tickets.csv...\n",
      " tickets.csv loaded (714 rows) \n",
      "\n",
      "\n",
      " Pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "# Pipeline config: list of tables/files to process\n",
    "sources = [\n",
    "    \"passengers.csv\",\n",
    "    \"cabins.csv\",\n",
    "    \"tickets.csv\"\n",
    "]\n",
    "\n",
    "print(\"===== Pipeline sources ===== \\n\")\n",
    "for source in sources:\n",
    "    print(f\"Loading {source}...\")\n",
    "    print(f\" {source} loaded ({len(df)} rows) \\n\")\n",
    "\n",
    "print(\"\\n Pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0758de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pipeline Started === \n",
      "\n",
      "Step 1 : passengers.csv\n",
      "passengers.csv loading\n",
      "passengers.csv loaded 714 rows \n",
      "\n",
      "Step 2 : cabins.csv\n",
      "cabins.csv loading\n",
      "cabins.csv loaded 714 rows \n",
      "\n",
      "Step 3 : tickets.csv\n",
      "tickets.csv loading\n",
      "tickets.csv loaded 714 rows \n",
      "\n",
      "\n",
      " Pipeline Completed\n"
     ]
    }
   ],
   "source": [
    "sources = [\n",
    "    \"passengers.csv\",\n",
    "    \"cabins.csv\",\n",
    "    \"tickets.csv\"\n",
    "]\n",
    "\n",
    "print(\"=== Pipeline Started === \\n\")\n",
    "\n",
    "for i , source in enumerate(sources):\n",
    "    print(f\"Step {i+1} : {source}\")\n",
    "    print(f\"{source} loading\")\n",
    "    print(f\"{source} loaded {len(df)} rows \\n\")\n",
    "\n",
    "print(\"\\n Pipeline Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2790a2c",
   "metadata": {},
   "source": [
    "Exercise 2: Loop over dict config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbbafa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    \"databases\" : {\n",
    "        \"SNOWFLAKE_WH\": {\"host\": \"snowflake.com\" , \"warehouse\": \"COMPUTE_WH\"},\n",
    "        \"POSTGRES_SALES\": {\"host\": \"pg.company.com\" , \"db\":\"sales\"}\n",
    "    },\n",
    "    \"files\": [\n",
    "        {\"path\" : \"titanic.csv\", \"rows_expected\": 891},\n",
    "        {\"path\" : \"orders.csv\", \"rows_expected\": 5000}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c774ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('SNOWFLAKE_WH', {'host': 'snowflake.com', 'warehouse': 'COMPUTE_WH'}), ('POSTGRES_SALES', {'host': 'pg.company.com', 'db': 'sales'})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config['databases'].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3189ee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extract step : looping over config ===\n",
      "Connecting to SNOWFLAKE_WH : snowflake.com\n",
      "Connecting to POSTGRES_SALES : pg.company.com\n",
      "\n",
      " === File Loads ===\n",
      " WARNING: LOW ROWS ==> titanic.csv\n",
      " Expected: 891 rows , Acutal: 714 rows\n",
      " WARNING: LOW ROWS ==> orders.csv\n",
      " Expected: 5000 rows , Acutal: 714 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Extract step : looping over config ===\")\n",
    "for db_nm , details in pipeline_config['databases'].items() :\n",
    "    print(f\"Connecting to {db_nm} : {details['host']}\")\n",
    "\n",
    "print(\"\\n === File Loads ===\")\n",
    "for file_info in pipeline_config[\"files\"]:\n",
    "    actual_rows = len(df)\n",
    "    if actual_rows >= file_info[\"rows_expected\"]:\n",
    "        print(f\"{file_info[\"path\"]} : {actual_rows} rows ==> COUNT OK\")\n",
    "    else:\n",
    "        print(f\" WARNING: LOW ROWS ==> {file_info[\"path\"]}\")\n",
    "        print(f\" Expected: {file_info['rows_expected']} rows , Acutal: {actual_rows} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae92b7",
   "metadata": {},
   "source": [
    "Exercise 3: Nested loop – tables → columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ebd56",
   "metadata": {},
   "source": [
    "Conditional Experession\n",
    "\n",
    "value_if_true if condition else value_if_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b14da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data quality scan (nested loops) ===\n",
      "\n",
      "Scanning passengers (3 cols):\n",
      "name: 0 nulls\n",
      "age: 0 nulls\n",
      "fare: 0 nulls\n",
      "\n",
      "Scanning cabins (3 cols):\n",
      "CabinId: N/A nulls\n",
      "Class: N/A nulls\n",
      "Deck: N/A nulls\n"
     ]
    }
   ],
   "source": [
    "tables = {\n",
    "    \"passengers\": df[[\"name\", \"age\", \"fare\"]].columns.tolist(),\n",
    "    \"cabins\": [\"CabinId\", \"Class\", \"Deck\"]\n",
    "}\n",
    "\n",
    "print(\"=== Data quality scan (nested loops) ===\")\n",
    "for table_name, columns in tables.items():\n",
    "    print(f\"\\nScanning {table_name} ({len(columns)} cols):\")\n",
    "    for col in columns:\n",
    "        col_nulls = df[col].isnull().sum() if col in df.columns else \"N/A\"\n",
    "        print(f\"{col}: {col_nulls} nulls\")\n",
    "        if col in df.columns and col_nulls > 100:\n",
    "            print(\"HIGH NULLS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85306720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
